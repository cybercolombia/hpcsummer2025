{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad706c1-2a27-4d9a-8199-d873fa9a935d",
   "metadata": {},
   "source": [
    "# <center>Introduction to MPI</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43df52f",
   "metadata": {},
   "source": [
    "# Shared Memory System vs Distributed Memory System \n",
    "\n",
    "## Shared memory Systems \n",
    "All compute elements shared access to same memory space, using two strategies of Memory Access\n",
    "\n",
    "### Unified Memory Access\n",
    "![Unified Memory Access](./img/shared_mem.gif)\n",
    "\n",
    "\n",
    "### Non-Unified Memory Access \n",
    "![Non-Unified  Memory Access (UMA)](./img/numa.gif)\n",
    "\n",
    "Both strategies running intra-node, with limited capacity to increase the computer processing capacity. **(Vertical Scaling)**\n",
    "\n",
    "When we need increase the capacity, need use a different approach, **(Horizontal Scaling)**\n",
    "\n",
    "##Distributed Memory System. \n",
    "\n",
    "Adding multiples nodes each one with your own memory space, and working like one compute unit, show the challenges of Distributed Computing\n",
    "\n",
    "![Distributed Memory System](./img/hybrid_mem.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu  # This command shows the CPU information and the Numa node information, put special attention to NUMA information\n",
    "!numctl -H # This command shows the NUMA information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04937204",
   "metadata": {},
   "source": [
    "## MPI (Message Passing Interface)\n",
    "\n",
    "### Overview\n",
    "The Message Passing Interface (MPI) is a standardized and portable message-passing system designed to function on parallel computing architectures. MPI is widely used for parallel programming in high-performance computing (HPC) environments.\n",
    "\n",
    "MPI addresses the message-passing parallel programming model: data is **moved from the address space** of one process to that of another process through cooperative operations on each process.\n",
    "\n",
    "### MPI Standard\n",
    "The MPI standard defines the syntax and semantics of library routines that can be used to write portable message-passing programs in C, C++, and Fortran. The most current version of MPI is MPI-3.1., but The MPI standard has gone through a number of revisions, with the most recent version being MPI-4.x but MPI-5.0 was approved on June 5 of 2025, please see [MPI Standard\n",
    "](https://www.mpi-forum.org/docs/)\n",
    "\n",
    "### MPI Implementations\n",
    "There are several implementations of the MPI standard. Two of the most widely used implementations are:\n",
    "- **MPICH**: A high-performance and widely portable implementation of MPI.\n",
    "- **INTELMPI**: Intel specific implementation \n",
    "- **OpenMPI**: An open-source MPI implementation that is developed and maintained by a consortium of academic, research, and industry partners.\n",
    "\n",
    "OpenMPI offer MPI Build Script for Linux Clusters, \n",
    "\n",
    "\n",
    "|Implementation |language   |ScriptName | Underlying Compiler|\n",
    "|  --- |    --- |   --- |   --- |\n",
    "|Open MPI       |\tC\t    | mpicc\t    |C compiler for loaded compiler package|\n",
    "|               |   C++\t| - mpiCC <br/> - mpic++ <br/>- mpicxx\t    |C++ compiler for loaded compiler package|\n",
    "|               |   Fortran\t|   -mpif77 <br/> - mpif90\t| Fortran77 compiler for loaded compiler package <br/>Fortran90 compiler for loaded compiler package. Points to mpifort.|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade9740c-2564-41a8-873d-8ea09a64b1e1",
   "metadata": {},
   "source": [
    "\n",
    "## Setting Up the Environment\n",
    "To start programming with MPI in C or C++, you need to have an MPI library installed. For this tutorial, we'll use OpenMPI. Below are the steps to install and compile MPI programs using GCC and OpenMPI.\n",
    "\n",
    "### Installation of OpenMPI\n",
    "You can install OpenMPI on a Unix-based system using a package manager. For example, on Ubuntu, you can use:\n",
    "```bash\n",
    "sudo apt-get update\n",
    "sudo apt-get install openmpi-bin openmpi-common libopenmpi-dev\n",
    "```\n",
    "***Note:***: All libs and wrappers are installing, please don't try it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469f2a1c-3aec-4088-b206-b18b06f7590e",
   "metadata": {},
   "source": [
    "## Compiling MPI Programs\n",
    "MPI programs are compiled using the `mpicc` or `mpiCC` compiler wrappers, which are part of the OpenMPI package. These wrappers call the underlying compiler (e.g., GCC) with the correct flags and libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12386848-de9d-4d2c-82fd-b5da38a0ef39",
   "metadata": {},
   "source": [
    "\n",
    "`hello_world.c`\n",
    "```C\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int world_rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "\n",
    "    int world_size;\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
    "\n",
    "    printf(\"Hello world from rank %d out of %d processors\\n\", world_rank, world_size);\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde65098-3bc1-491f-b679-6559006ab8e1",
   "metadata": {},
   "source": [
    "Let's compile the program using mpi wrapper compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe27dc85-f282-4452-bb2a-0ad905e2b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc code/hello_world.c -o code/hello_world "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff39421-c712-4aa0-95b0-a75686771958",
   "metadata": {},
   "source": [
    "which should create an executable file called `hello_world`. And now execute the program, and see in details the runtime execution using `mpirun` command or summit it to current cluster using a Slurm Job Manager with command `srun` and the parameter used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94e4c48a-2bf7-4664-ad86-812a5b2e1af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am rank 3 out of 4 on host ss-19 \n",
      "I am rank 1 out of 4 on host ss-18 \n",
      "I am rank 2 out of 4 on host ss-19 \n",
      "I am rank 0 out of 4 on host ss-18 \n"
     ]
    }
   ],
   "source": [
    "#!mpirun -np 4 ./hello_world # This example uses 4 processes on the same machine \n",
    "!srun -N 2 -n 4 --ntasks-per-node=2 ./hello_world  # This example uses 4 processes on 2 nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5502f-f786-4090-aba6-618479086184",
   "metadata": {},
   "source": [
    "Note that the execution block is enclosed in a function called `main()`, which returns the value 0 if it is completed successfully. The declaration of `main()` is mandatory in C/C++."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a86f4b-2a14-4838-9146-371b8f1b6b90",
   "metadata": {},
   "source": [
    "\n",
    "## Unit 1: MPI Basics\n",
    "\n",
    "### Subtopic 1.1: MPI Initialization and Finalization\n",
    "#### Explanation\n",
    "- **MPI_Init**: Initializes the MPI execution environment.\n",
    "- **MPI_Finalize**: Terminates the MPI execution environment.\n",
    "\n",
    "#### Example: Initialization and Finalization basic.cpp\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "    //During MPI_Init, all of MPIâ€™s global and internal variables are constructed. \n",
    "    printf(\"MPI environment initialized.\\n\");\n",
    "    MPI_Get_version(&version, &subversion);\n",
    "    //Used to capture the Version and Release of Implementation\n",
    "    printf(\"MPI Version: %d.%d\\n\", version, subversion);\n",
    "    MPI_Finalize();\n",
    "    //MPI_Finalize is used to clean up the MPI environment. No more MPI calls can be made after this one. \n",
    "    printf(\"MPI environment finalized.\\n\");\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17776b3b-83de-4807-84ef-d77c9058a991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI environment initialized.\n",
      "MPI Version: 3.1\n",
      "MPI environment finalized.\n",
      "MPI environment initialized.\n",
      "MPI Version: 3.1\n",
      "MPI environment finalized.\n",
      "MPI environment initialized.\n",
      "MPI Version: 3.1\n",
      "MPI environment finalized.\n",
      "MPI environment initialized.\n",
      "MPI Version: 3.1\n",
      "MPI environment finalized.\n"
     ]
    }
   ],
   "source": [
    "!mpicc code/basic_mpi.cpp -o code/unit_1_basic\n",
    "!srun -N 2 -n 4 --ntasks-per-node=2 ./code/unit_1_basic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbc13076-2069-4430-8657-bede644744cd",
   "metadata": {},
   "source": [
    "## Basic Anatomy of MPI Program. \n",
    "\n",
    "### Simple Communication.\n",
    "\n",
    "![Simple MPI Send and Receive ](https://cvw.cac.cornell.edu/mpip2p/intro/SimpleSendAndRecv.gif)\n",
    "\n",
    "\n",
    "```c\n",
    "\n",
    "\n",
    "// include libs \n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "\n",
    "//inicialization \n",
    "    MPI_Init(&argc, &argv);\n",
    "//get the number of rank and the size of Communicator \n",
    "  int world_rank;\n",
    "  int world_size;  \n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
    "//\n",
    "  char processor_name[MPI_MAX_PROCESSOR_NAME];\n",
    "  int name_len;\n",
    "  MPI_Get_processor_name(processor_name, &name_len);\n",
    "  printf(\"Running on node: %s\\n\",processor_name);\n",
    "  printf(\"Number of ranks: %d\\n\",   world_size);\n",
    "//evaluation of rank id to take action\n",
    "// Normaly rank_id=0 mean that is the master process\n",
    "    if (world_rank == 0) {\n",
    "        int data = 100;\n",
    "        //Send data to communicators \n",
    "        MPI_Send(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n",
    "        printf(\"Process 0 sent data %d to process 1\\n\", data);\n",
    "    } else if (world_rank == 1) {\n",
    "        int data;\n",
    "        //receive data \n",
    "        MPI_Recv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "        printf(\"Process 1 received data %d from process 0\\n\", data);\n",
    "    }\n",
    "  MPI_Finalize();\n",
    "  printf(\"MPI environment finalized.\\n\");\n",
    "}\n",
    "```\n",
    "\n",
    "### Types of communications. \n",
    "\n",
    "In MPI (Message Passing Interface), communications are divided into two main **categories**: point-to-point and collective communications. Each of these types has subtypes depending on synchronization, buffering, and topology\n",
    "\n",
    "\n",
    "### Point-to-Point Communication\n",
    "#### Blocking communication\n",
    "- **MPI_Send**: Sends a message to another process.\n",
    "- **MPI_Recv**: Receives a message from another process.\n",
    "### Simple Communication.\n",
    "\n",
    "![Simple MPI Blocking Communications ](https://cvw.cac.cornell.edu/mpip2p/communication-modes/SynchSendAndRecv.gif)\n",
    "\n",
    "\n",
    "\n",
    "#### Example: Send and Receive\n",
    "```c\n",
    "#include <stdio.h>\n",
    "#include \"mpi.h\"\n",
    "\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  MPI_Init(&argc, &argv); // alt.: NULL,NULL\n",
    "  printf(\"MPI environment initialized.\\n\");\n",
    "  int size, rank;\n",
    "\n",
    "  // Copy the communicator\n",
    "\n",
    "  int world_rank;\n",
    "  int world_size;\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
    "  char processor_name[MPI_MAX_PROCESSOR_NAME];\n",
    "  int name_len;\n",
    "  MPI_Get_processor_name(processor_name, &name_len);\n",
    "  printf(\"Running on node: %s\\n\",processor_name);\n",
    "  printf(\"Number of ranks: %d\\n\",   world_size);\n",
    "    if (world_rank == 0) {\n",
    "        int data = 100;\n",
    "        double start = MPI_Wtime();\n",
    "        MPI_Send(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n",
    "        double end = MPI_Wtime();\n",
    "        printf(\"Time taken on send : %f seconds\\n\", end - start);\n",
    "        printf(\"Process 0 sent data %d to process 1\\n\", data);\n",
    "    } else if (world_rank == 1) {\n",
    "        int data;\n",
    "        double start = MPI_Wtime();\n",
    "        MPI_Recv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "        double end = MPI_Wtime();\n",
    "        printf(\"Time taken on received : %f seconds\\n\", end - start);\n",
    "        printf(\"Process 1 received data %d from process 0\\n\", data);\n",
    "    }\n",
    "  MPI_Finalize();\n",
    "  printf(\"MPI environment finalized.\\n\");\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "This type of communications is **blocking**, because Sender blocks until buffer is safe to reuse; receiver blocks until message is received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a75ca8e4-9566-497a-ad35-945e1b89201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI environment initialized.\n",
      "Running on node: ss-19\n",
      "Number of ranks: 4\n",
      "MPI environment finalized.\n",
      "MPI environment initialized.\n",
      "Running on node: ss-18\n",
      "Number of ranks: 4\n",
      "Time taken on received : 0.000238 seconds\n",
      "Process 1 received data 100 from process 0\n",
      "MPI environment finalized.\n",
      "MPI environment initialized.\n",
      "Running on node: ss-19\n",
      "Number of ranks: 4\n",
      "MPI environment finalized.\n",
      "MPI environment initialized.\n",
      "Running on node: ss-18\n",
      "Number of ranks: 4\n",
      "Time taken on send : 0.000020 seconds\n",
      "Process 0 sent data 100 to process 1\n",
      "MPI environment finalized.\n"
     ]
    }
   ],
   "source": [
    "!mpicc code/blocking_comm.cpp -o code/blocking_comm\n",
    "!srun -N 2 -n 4 --ntasks-per-node=2 ./code/blocking_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18164935-a968-4745-b2e1-f5fb5cdb8b12",
   "metadata": {},
   "source": [
    "### Buffered Send\n",
    "The blocking buffered send MPI_Bsend copies the data from the message buffer to a user-supplied buffer and then returns. The data will be copied from the user-supplied buffer over the network once the \"ready to receive\" notification has arrived.\n",
    "![Simple MPI Buffered Send Communications ](https://cvw.cac.cornell.edu/mpip2p/communication-modes/BuffSendAndRecv.gif)\n",
    "\n",
    "```c\n",
    "#include <stdio.h>\n",
    "#include <cstdlib>\n",
    "#include \"mpi.h\"\n",
    "\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  MPI_Init(&argc, &argv); // alt.: NULL,NULL\n",
    "  printf(\"MPI environment initialized.\\n\");\n",
    "  int size, rank;\n",
    "\n",
    "  // Copy the communicator\n",
    "\n",
    "  int world_rank;\n",
    "  int world_size;\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
    "  char processor_name[MPI_MAX_PROCESSOR_NAME];\n",
    "  int name_len;\n",
    "  MPI_Get_processor_name(processor_name, &name_len);\n",
    "  printf(\"Running on node: %s (rank %d of %d)\\n\", processor_name, world_rank, world_size);\n",
    "  // Declare the times \n",
    "        double start_i, end_i;\n",
    "           \n",
    "    if (world_size < 2) {\n",
    "        if (world_rank == 0) {\n",
    "            fprintf(stderr, \"This example needs at least two processes.\\n\");\n",
    "        }\n",
    "        MPI_Finalize();\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    if (world_rank == 0) {\n",
    "        int data = 42;\n",
    "\n",
    "        // Calculate size needed for the buffer\n",
    "        int bufsize;\n",
    "        MPI_Pack_size(1, MPI_INT, MPI_COMM_WORLD, &bufsize);\n",
    "        // Calculate the Buffer Size. \n",
    "        // MPI_BSEND_OVERHEAD represents the size, in bytes, of the memory overhead generated \n",
    "        // everytime an MPI_Bsend or MPI_Ibsend is issued.\n",
    "        bufsize += MPI_BSEND_OVERHEAD;\n",
    "\n",
    "        // Allocate and attach buffer\n",
    "        void* buffer = malloc(bufsize);\n",
    "        if (buffer == NULL) {\n",
    "            fprintf(stderr, \"Could not allocate buffer\\n\");\n",
    "            MPI_Abort(MPI_COMM_WORLD, 1);\n",
    "        }\n",
    "        // Setuo the buffer \n",
    "        MPI_Buffer_attach(buffer, bufsize);\n",
    "         \n",
    "        // Send data using buffered send\n",
    "        double start = MPI_Wtime();\n",
    "        MPI_Bsend(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n",
    "        double end = MPI_Wtime();\n",
    "        printf(\"Time taken: %f seconds\\n\", end - start);\n",
    "        printf(\"Process 0 sent buffered data %d to process 1\\n\", data);\n",
    "\n",
    "        // Detach buffer (waits for completion of internal copies)\n",
    "        void* detached_buffer;\n",
    "        int detached_size;\n",
    "        MPI_Buffer_detach(&detached_buffer, &detached_size);\n",
    "        free(detached_buffer);\n",
    "    }\n",
    "\n",
    "    else if (world_rank == 1) {\n",
    "        int received;\n",
    "        start_i = MPI_Wtime();\n",
    "        MPI_Recv(&received, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "        end_i = MPI_Wtime();\n",
    "        printf(\"Time taken: %f seconds\\n\", end_i - start_i);\n",
    "        printf(\"Process 1 received data %d from process 0\\n\", received);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "## Differences\n",
    "\n",
    "The **real difference** between **buffered** and **non-buffered** communication in MPI lies in **how the send operation completes** and **who manages the memory and synchronization of the data transfer**.\n",
    "\n",
    "---\n",
    "\n",
    "### a. Conceptual Differences\n",
    "\n",
    "| Aspect                 | **Buffered Communication (`MPI_Bsend`)**                                         | **Standard Communication (`MPI_Send`)**                                                    |\n",
    "| ---------------------- | -------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ |\n",
    "| **Completion**         | Returns once the user-provided buffer is **copied** into an internal MPI buffer. | May block until the receiver has **started receiving** or MPI has safely handled the data. |\n",
    "| **Buffer Management**  | **User** must allocate and manage a buffer via `MPI_Buffer_attach`.              | **MPI implementation** decides whether to use buffering or synchronous send.               |\n",
    "| **Deadlock Potential** | Lower risk; sender does not wait for receiver.                                   | Higher risk; sender may block if receiver is not ready (especially in circular waits).     |\n",
    "| **Use Case**           | Useful when you need to **avoid blocking** without using non-blocking functions. | Suitable for simple, tightly-coupled communication patterns.                               |\n",
    "| **Control Level**      | Medium-level; user controls when to buffer, how much.                            | Low-level; MPI decides buffering or synchronization strategy.                              |\n",
    "\n",
    "---\n",
    "\n",
    "### b. Performance Considerations\n",
    "\n",
    "| Factor               | Buffered Send (`MPI_Bsend`)                                                        | Standard Send (`MPI_Send`)                                                |\n",
    "| -------------------- | ---------------------------------------------------------------------------------- | ------------------------------------------------------------------------- |\n",
    "| **Latency**          | Typically **higher** (extra memory copy).                                          | May be **lower** if receiver is ready.                                    |\n",
    "| **Throughput**       | May degrade with large messages or limited buffer size.                            | Can be optimized by MPI via protocol (eager vs rendezvous).               |\n",
    "| **Memory Footprint** | Requires explicit buffer allocation by the user.                                   | Depends on internal MPI behavior (may still use buffers internally).      |\n",
    "| **Scalability**      | Less scalable due to user-managed buffers, especially with many messages or ranks. | More scalable when combined with non-blocking (`Isend/Irecv`) techniques. |\n",
    "\n",
    "---\n",
    "### c. Monitor System Resources\n",
    "\n",
    "Use tools like:\n",
    "\n",
    "* `valgrind` or `gprof` to check memory usage.\n",
    "* `perf`, `mpiP`, or `TAU` for MPI event profiling.\n",
    "* `top` / `htop` / `mpstat` for CPU and memory under pressure.\n",
    "\n",
    "### Final Insight\n",
    "\n",
    "In practice:\n",
    "\n",
    "* Prefer **non-blocking (`Isend`/`Irecv`)** for high performance and overlapping communication/computation.\n",
    "* Use `Bsend` **only when non-blocking is not viable** and you want simpler code **without risk of deadlock**.\n",
    "* Use `Send` when communication is simple and tightly coordinated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "889ff5f9-4a9f-43b0-bdd8-4bc74102e8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI environment initialized.\n",
      "Running on node: ss-18 (rank 1 of 4)\n",
      "Time taken: 0.000270 seconds\n",
      "Process 1 received data 42 from process 0\n",
      "MPI environment initialized.\n",
      "Running on node: ss-19 (rank 3 of 4)\n",
      "MPI environment initialized.\n",
      "Running on node: ss-18 (rank 0 of 4)\n",
      "Time taken: 0.000027 seconds\n",
      "Process 0 sent buffered data 42 to process 1\n",
      "MPI environment initialized.\n",
      "Running on node: ss-19 (rank 2 of 4)\n"
     ]
    }
   ],
   "source": [
    "!mpicc code/buffered_comm.cpp -o code/buffered_comm\n",
    "!srun -N 2 -n 4 --ntasks-per-node=2 code/buffered_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca63b290-26c1-4d9b-889c-c8022f10bf4c",
   "metadata": {},
   "source": [
    "### Benchmarking of both approach \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4be9df7f-8a54-4c2c-8da0-f2f8dc01db40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark on 2 MPI ranks...\n",
      "[MPI_Send] size: 1 bytes, avg latency: 0.410186 us\n",
      "[MPI_Send] size: 2 bytes, avg latency: 0.290445 us\n",
      "[MPI_Send] size: 4 bytes, avg latency: 0.291977 us\n",
      "[MPI_Send] size: 8 bytes, avg latency: 0.287632 us\n",
      "[MPI_Send] size: 16 bytes, avg latency: 0.331509 us\n",
      "[MPI_Send] size: 32 bytes, avg latency: 0.322792 us\n",
      "[MPI_Send] size: 64 bytes, avg latency: 0.319868 us\n",
      "[MPI_Send] size: 128 bytes, avg latency: 0.427865 us\n",
      "[MPI_Send] size: 256 bytes, avg latency: 0.505741 us\n",
      "[MPI_Send] size: 512 bytes, avg latency: 1.677368 us\n",
      "[MPI_Send] size: 1024 bytes, avg latency: 1.889915 us\n",
      "[MPI_Send] size: 2048 bytes, avg latency: 2.456696 us\n",
      "[MPI_Send] size: 4096 bytes, avg latency: 6.586184 us\n",
      "[MPI_Send] size: 8192 bytes, avg latency: 4.932796 us\n",
      "[MPI_Send] size: 16384 bytes, avg latency: 5.934676 us\n",
      "[MPI_Send] size: 32768 bytes, avg latency: 6.455069 us\n",
      "[MPI_Send] size: 65536 bytes, avg latency: 8.161791 us\n",
      "[MPI_Send] size: 131072 bytes, avg latency: 12.611481 us\n",
      "[MPI_Send] size: 262144 bytes, avg latency: 21.147180 us\n",
      "[MPI_Send] size: 524288 bytes, avg latency: 38.548576 us\n",
      "[MPI_Send] size: 1048576 bytes, avg latency: 71.344961 us\n",
      "[MPI_Bsend] size: 1 bytes, avg latency: 0.083910 us\n",
      "[MPI_Bsend] size: 2 bytes, avg latency: 0.080336 us\n",
      "[MPI_Bsend] size: 4 bytes, avg latency: 0.090978 us\n",
      "[MPI_Bsend] size: 8 bytes, avg latency: 0.076831 us\n",
      "[MPI_Bsend] size: 16 bytes, avg latency: 0.092817 us\n",
      "[MPI_Bsend] size: 32 bytes, avg latency: 0.088175 us\n",
      "[MPI_Bsend] size: 64 bytes, avg latency: 0.092656 us\n",
      "[MPI_Bsend] size: 128 bytes, avg latency: 0.124041 us\n",
      "[MPI_Bsend] size: 256 bytes, avg latency: 0.153917 us\n",
      "[MPI_Bsend] size: 512 bytes, avg latency: 0.427885 us\n",
      "[MPI_Bsend] size: 1024 bytes, avg latency: 2.736480 us\n",
      "[MPI_Bsend] size: 2048 bytes, avg latency: 0.398729 us\n",
      "[MPI_Bsend] size: 4096 bytes, avg latency: 3.221341 us\n",
      "[MPI_Bsend] size: 8192 bytes, avg latency: 4.667434 us\n",
      "[MPI_Bsend] size: 16384 bytes, avg latency: 9.235142 us\n",
      "[MPI_Bsend] size: 32768 bytes, avg latency: 18.804194 us\n",
      "[MPI_Bsend] size: 65536 bytes, avg latency: 38.193063 us\n",
      "[MPI_Bsend] size: 131072 bytes, avg latency: 77.467859 us\n",
      "[MPI_Bsend] size: 262144 bytes, avg latency: 153.758167 us\n",
      "[MPI_Bsend] size: 524288 bytes, avg latency: 309.256195 us\n",
      "[MPI_Bsend] size: 1048576 bytes, avg latency: 723.514828 us\n"
     ]
    }
   ],
   "source": [
    "!mpicc -O2  code/mpi_bench_latency.cpp -o code/mpi_bench_latency\n",
    "!srun -N 1 -n 2  code/mpi_bench_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b564dade-fa23-4f76-9999-203953f3eb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for ss2: Error:\n",
      "Access to performance monitoring and observability operations is limited.\n",
      "Consider adjusting /proc/sys/kernel/perf_event_paranoid setting to open\n",
      "access to performance monitoring and observability operations for processes\n",
      "without CAP_PERFMON, CAP_SYS_PTRACE or CAP_SYS_ADMIN Linux capability.\n",
      "More information can be found at 'Perf events and tool security' document:\n",
      "https://www.kernel.org/doc/html/latest/admin-guide/perf-security.html\n",
      "perf_event_paranoid setting is 4:\n",
      "  -1: Allow use of (almost) all events by all users\n",
      "      Ignore mlock limit after perf_event_mlock_kb without CAP_IPC_LOCK\n",
      ">= 0: Disallow raw and ftrace function tracepoint access\n",
      ">= 1: Disallow CPU event access\n",
      ">= 2: Disallow kernel profiling\n",
      "To make the adjusted perf_event_paranoid setting permanent preserve it\n",
      "in /etc/sysctl.conf (e.g. kernel.perf_event_paranoid = <setting>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# only if whe have the permission over the kernel metrics \n",
    "!perf stat -e cache-misses,cache-references,context-switches,page-faults,cycles,instructions \\\n",
    "  mpirun -np 2 ./mpi_bench_latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deecb66f-e718-48d9-8988-e2e3be40b451",
   "metadata": {},
   "source": [
    "\n",
    "### Non-Blocking communication\n",
    "\n",
    "MPI provides both blocking and nonblocking point-to-point communication:\n",
    "\n",
    "**Blocking communication** means that the process waits to ensure the message data have achieved a particular state before processing can continue.\n",
    "**Nonblocking communication** means that the processor merely requests to start an operation and continues processing.\n",
    "\n",
    "**Nonblocking** calls merely initiate the communication process. The status of the data transfer, and the success of the communication, must be verified at a later point in the program. The purpose of a nonblocking send is mostly to notify the system of the existence of an outgoing message: the actual transfer might take place later. It is up to the programmer to keep the send buffer intact until it can be verified that the message has actually been copied someplace else. Likewise, a nonblocking receive signals the system that a buffer is prepared for an incoming message, without waiting for the actual data to arrive\n",
    "\n",
    "\n",
    "- **MPI_ISend/MPI_IRecv**: Initiates communication and returns immediately;requires MPI_Wait or MPI_Test to complete.'\n",
    "- **MPI_Wait / MPI_Waitall**  Blocks the caller until the operation associated with the **MPI_Request** is complete\n",
    "\n",
    "```c\n",
    "#include <mpi.h> //nonblocking_comm.cpp\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int world_rank;\n",
    "    int world_size;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
    "\n",
    "    char processor_name[MPI_MAX_PROCESSOR_NAME];\n",
    "    int name_len;\n",
    "    MPI_Get_processor_name(processor_name, &name_len);\n",
    "\n",
    "    printf(\"Running on node: %s\\n\", processor_name);\n",
    "    printf(\"Number of ranks: %d\\n\", world_size);\n",
    "\n",
    "    if (world_rank == 0) {\n",
    "        int data = 100;\n",
    "        MPI_Request request;\n",
    "        MPI_Isend(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &request);  // Non-blocking send\n",
    "        MPI_Wait(&request, MPI_STATUS_IGNORE);  // Wait for completion\n",
    "        printf(\"Process 0 sent data %d to process 1\\n\", data);\n",
    "    } else if (world_rank == 1) {\n",
    "        int data;\n",
    "        MPI_Request request;\n",
    "        MPI_Irecv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &request);  // Non-blocking receive\n",
    "        MPI_Wait(&request, MPI_STATUS_IGNORE);  // Wait for completion\n",
    "        printf(\"Process 1 received data %d from process 0\\n\", data);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "    printf(\"MPI environment finalized.\\n\");\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68dbbcac-fdd3-4938-b2cc-098ad3e54278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kcode/nonblocking_comm.cpp.cpp: No such file or directory\n",
      "compilation terminated.\n",
      "slurmstepd-ss-19: error: execve(): /home/ss2/hpcsummer2025/material/Track3/./code/nonblocking_comm: No such file or directory\n",
      "slurmstepd-ss-18: error: execve(): /home/ss2/hpcsummer2025/material/Track3/./code/nonblocking_comm: No such file or directory\n",
      "slurmstepd-ss-19: error: execve(): /home/ss2/hpcsummer2025/material/Track3/./code/nonblocking_comm: No such file or directory\n",
      "slurmstepd-ss-18: error: execve(): /home/ss2/hpcsummer2025/material/Track3/./code/nonblocking_comm: No such file or directory\n",
      "srun: error: ss-19: tasks 2-3: Exited with exit code 2\n",
      "srun: error: ss-18: tasks 0-1: Exited with exit code 2\n"
     ]
    }
   ],
   "source": [
    "!mpicxx code/nonblocking_comm.cpp -o code/nonblocking_comm\n",
    "!srun -N 2 -n 4 --ntasks-per-node=2 ./code/nonblocking_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3139e2e9-ca36-461d-8001-afc2a694cccc",
   "metadata": {},
   "source": [
    "### Exercise No 1\n",
    "Create a program using blocking and nonblocking communication to create a distributed sum, compile it and running on two nodes each one with 4 process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126d605-75d3-494c-b0ff-63767a6fbcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc exercises/exercise1.cpp -o exercises/exercise1 #for blocking communication\n",
    "!srun -N #TODO# --ntasks-per-node=2 ./exercises/exercise1\n",
    "!mpicc exercises/exercise2.cpp -o exercises/exercise2 #for nonblocking communication\n",
    "!srun -N #TODO# --ntasks-per-node=2 ./exercises/exercise2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3de1711f-2fc6-4c35-8668-f2fe1c1ed768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum computed at root: 21\n"
     ]
    }
   ],
   "source": [
    "!mpic++ solutions/exercise2.cpp -o solutions/exercise2\n",
    "!srun -N 3 -n 6 --ntasks-per-node=2 solutions/exercise2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b1fd2-5722-4f22-b0b1-9b87edb0f5da",
   "metadata": {},
   "source": [
    "\n",
    "## MPI Communicators\n",
    "\n",
    "Collective communication involves all the processes in a **communicator**. The purpose of collective communication is to manipulate a shared piece or set of information. The collective communication routines were built upon point-to-point communication routines. You could build your own collective communication routines in this way, but it might involve a lot of tedious work and might not be as efficient.\n",
    "\n",
    "Parallel applications in a **distributed memory environment** sometimes require explicit or implicit synchronization. Like other message-passing libraries, MPI provides a routine, **MPI_BARRIER**, to synchronize all processes within a communicator. A barrier is simply a synchronization primitive. Any process calling it will be blocked until all the processes within the group have called it. Once all the processes in the communicator group have reached the barrier, the function will return, and all processes in the group can continue.\n",
    "\n",
    "MPI provides **three categories** of collective data-movement routines in which one process either sends to or receives from **all processes**: broadcast, gather, and scatter. There are also **allgather and alltoall routines**, which require all processes both to send and receive data. The gather, scatter, allgather, and alltoall routines have variable-data versions. For their variable data (\"v\") versions, each process can send and/or receive a different number of elements. The list of MPI collective data movement routines are:\n",
    "\n",
    "broadcast\n",
    "gather, gatherv\n",
    "scatter, scatterv\n",
    "allgather, allgatherv\n",
    "alltoall, alltoallv\n",
    "\n",
    "#### Explanation\n",
    "- **MPI_COMM_WORLD**: Default communicator including all processes.\n",
    "- **MPI_Comm_size**: Determines the size of the group associated with a communicator.\n",
    "- **MPI_Comm_rank**: Determines the rank of the calling process in the communicator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d3e4c",
   "metadata": {},
   "source": [
    "\n",
    "## MPI Collective Communication\n",
    "\n",
    "The Type of Collective communication on MPI are:\n",
    "![Collective](./img/collective_comm.gif)\n",
    "<br>\n",
    "![MPI_AllGather](https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/allgather.png)\n",
    "\n",
    "### Broadcast\n",
    "#### Explanation\n",
    "- **MPI_Bcast**: Broadcasts a message from the process with rank \"root\" to all other processes in the communicator.\n",
    "\n",
    "#### Example: Broadcast\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    int world_rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "\n",
    "    int data = 0;\n",
    "    if (world_rank == 0) {\n",
    "        data = 100;\n",
    "    }\n",
    "    MPI_Bcast(&data, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "    printf(\"Process %d received data %d\\n\", world_rank, data);\n",
    "\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58da59d",
   "metadata": {},
   "source": [
    "### MPI  Data Types\n",
    "\n",
    "To share data between nodes, is required use same data types, to cast values and manipulate side to side. MPI predefines its primitive data types:\n",
    "\n",
    "\n",
    "|C Data Types 1| C Data Types 2|\n",
    "|   ---  |  --- |\t\n",
    "|MPI_CHAR<br/>MPI_WCHAR<br/>MPI_SHORT<br/>MPI_INT<br/>MPI_LONG<br/>MPI_LONG_LONG_INT<br/>MPI_LONG_LONG<br/>MPI_SIGNED_CHAR<br/>MPI_UNSIGNED_CHAR<br/>MPI_UNSIGNED_SHORT<br/>MPI_UNSIGNED_LONG<br/>MPI_UNSIGNED<br/>MPI_FLOAT<br/>MPI_DOUBLE<br/>MPI_LONG_DOUBLE|MPI_C_COMPLEX<br/>MPI_C_FLOAT_COMPLEX<br/>MPI_C_DOUBLE_COMPLEX<br/>MPI_C_LONG_DOUBLE_COMPLEX<br/>MPI_C_BOOL<br/>MPI_LOGICAL<br/>MPI_C_LONG_DOUBLE_COMPLEX<br/>MPI_INT8_T<br/>MPI_INT16_T<br/>MPI_INT32_T<br/>MPI_INT64_T<br/>MPI_UINT8_T<br/>MPI_UINT16_T<br/>MPI_UINT32_T<br/>MPI_UINT64_T<br/>MPI_BYTE<br/>MPI_PACKED|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0303c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|MPI Reduction Operation|\tC Data Types\t|\n",
    "|   --- |   --- |\n",
    "|MPI_MAX|\tmaximum\t|integer, float\t|\n",
    "|MPI_MIN|\tminimum\tinteger, float\t|\n",
    "|MPI_SUM|\tsum\t|integer, float\t|\n",
    "|MPI_PROD|\tproduct\t|integer, float\t|\n",
    "|MPI_LAND|\tlogical AND|\tinteger\t|\n",
    "|MPI_BAND|\tbit-wise AND|integer MPI_BYTE|\t\n",
    "|MPI_LOR|\tlogical OR|\tinteger\t|\n",
    "|MPI_BOR|\tbit-wise OR\tinteger, MPI_BYTE|\t\n",
    "|MPI_LXOR|\tlogical XOR\t|integer\t\n",
    "|MPI_BXOR|\tbit-wise XOR\t|integer, MPI_BYTE|\n",
    "|MPI_MAXLOC|\tmax value and location\t|float, double and long double|\t\n",
    "|MPI_MINLOC|\tmin value and location\t|float, double and long double|\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19827b28-f0ac-4c58-a8db-b964ab2a050a",
   "metadata": {},
   "source": [
    "### Exercise No 2. \n",
    "\n",
    "Please compile  with support for MPI and Submit it to Job Manager to compile and run the example of `distributed_sum.c`, use several configuration to runtime: Example 2 nodes with 2 Process per node,  with 4 process per node, 3, 4 Nodes too.  Please comment the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d6161-c887-4b63-a1a1-d3043f44ec01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
